{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with `dm_control`\n",
    "\n",
    "Here are some experiments using deep Q-Learning to solve simple continuous control tasks. I implemented [the original DQN](https://www.nature.com/articles/nature14236) and used it to solve several tasks in the [DeepMind Control Suite](https://arxiv.org/abs/1801.00690). The key innovations from DQN are:\n",
    "\n",
    "1. Maintain a replay buffer of experiences from which minibatches are randomly drawn during training. This decreases correlations in the training data, thereby reducing variance in the updates.\n",
    "2. Keep an additional Q network for calculating targets that is an 'outdated' version of the main Q network. Every `q_update_interval` updates the weights are copied from the main to the target q network. Updates are more stable because the target network is updated less frequently.\n",
    "\n",
    "Q-Learning takes the max across actions, which is not ideal for continuous action spaces. In this implementation the action space is discretized, such that each action dimension can take a value in `linspace(action_min, actions_max, action_grid)`, where `action_grid=2` for this demo. The full action space is the cartesian product of the vectors for each dimension.\n",
    "\n",
    "The demo is organized as follows:\n",
    "1. **setup**\n",
    "2. **solving tasks:** cartpole (balance+swingup), ball in cup, pendulum\n",
    "3. **double DQN:** I implement [Double Q-Learning](https://arxiv.org/abs/1509.06461) and test whether it increases the accuracy of action-value estimates.\n",
    "4. **encouraging exploration:** To increase exploration I used a simple trick to encourage optimism in the face of uncertainty. Namely, I pretrained the network to output optimistic action-values across the state-space, which encourages exploration in the early phases of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup\n",
    "\n",
    "\n",
    " The heart of the algorithm can be found in:\n",
    "- [`train_utils.train`](https://github.com/richard-warren/rl_sandbox/blob/e56c44d74ddd47cbd6c2dc37753ba95896f9b81d/dm_control_tests/train_utils.py#L87), which trains an [`Agent`](https://github.com/richard-warren/rl_sandbox/blob/e56c44d74ddd47cbd6c2dc37753ba95896f9b81d/dm_control_tests/agents.py#L18) given an agent and a `dm_control` environment. \n",
    "- [`Agent.update`](https://github.com/richard-warren/rl_sandbox/blob/e56c44d74ddd47cbd6c2dc37753ba95896f9b81d/dm_control_tests/agents.py#L67), which selects minibatches and performs network updates.\n",
    "\n",
    "To increase training speed I found it helpful to:\n",
    "- *Train on the CPU rather than GPU.* The Q network is very small. My CPU was faster than the GPU unless batch sizes were really large.\n",
    "- *Perform forward passes on Numpy*. Network forward passes ended up being much faster with numpy than Tensorflow (again, unless batch sizes were really large).\n",
    "- *Train multiple agents in parallel*. Training results could be somewhat idiosyncratic even with the same hyperparameters, so I train 12 agents in parallel to make sure the results are robust.\n",
    "\n",
    "Below are some utility functions for plotting performance and showing rollouts for trained agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dm_control_tests import train_utils, plot_utils\n",
    "from dm_control_tests.agents import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from dm_control import suite\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "train_utils.disable_gpu()\n",
    "\n",
    "# plot performance over training, averaged across agents\n",
    "def plot_performance(x, data):\n",
    "    ax = plt.axes(xlabel='episode', ylabel='return',\n",
    "                  xlim=(x[0],x[-1]), ylim=(0,1000))\n",
    "    data = np.array(data)\n",
    "    if data.ndim==2:\n",
    "        mean = data.mean(0)\n",
    "        std = data.std(0)\n",
    "        ax.plot(x, data.T, color=(0,0,0), alpha=.15)\n",
    "        ax.plot(x, mean)\n",
    "        ax.fill_between(x, mean+std, mean-std, alpha=.15)\n",
    "    else:\n",
    "        ax.plot(x, data)\n",
    "\n",
    "# show a rollout for the agent with the best performance at the end of training\n",
    "def show_best_agent_rollout(agents_dir, framerate=30, epsilon=.05):\n",
    "    with open(os.path.join(agents_dir, 'training_data'), 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "    best_agent = np.argmax(training_data['avg_returns'][:,-1])\n",
    "    agent, metadata = train_utils.load_agent(\n",
    "        os.path.join(agents_dir, 'agent{:03d}'.format(best_agent)))\n",
    "    env = suite.load(*metadata['domain_and_task'])\n",
    "    return plot_utils.show_rollout_jupyter(agent, env, epsilon=epsilon, framerate=framerate)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMIMwli/Fl2rto2fCxZqS+/",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dqn_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
