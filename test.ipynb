{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVTEfxM1tmDu"
   },
   "source": [
    "# DQN with `dm_control`\n",
    "\n",
    "Here are some experiments using deep Q-Learning to solve simple continuous control tasks. I implemented [the original DQN](https://www.nature.com/articles/nature14236) and used it to solve several tasks in the [DeepMind Control Suite](https://arxiv.org/abs/1801.00690). The key innovations from DQN are:\n",
    "\n",
    "1. Maintain a replay buffer of experiences from which minibatches are randomly drawn during training. This decreases correlations in the training data, thereby reducing variance in the updates.\n",
    "2. Keep an additional Q network for calculating targets that is an 'outdated' version of the main Q network. Every `q_update_interval` updates the weights are copied from the main to the target q network. Updates are more stable because the target network is updated less frequently.\n",
    "\n",
    "Q-Learning takes the max across actions, which is not ideal for continuous action spaces. In this implementation the action space is discretized, such that each action dimension can take a value in `linspace(action_min, actions_max, action_grid)`, where `action_grid=2` for this demo. The full action space is the cartesian product of the vectors for each dimension.\n",
    "\n",
    "The demo is organized as follows:\n",
    "1. **setup**\n",
    "2. **solving tasks:** cartpole (balance+swingup), ball in cup, pendulum\n",
    "3. **double DQN:** I implement [Double Q-Learning](https://arxiv.org/abs/1509.06461) and test whether it increases the accuracy of action-value estimates.\n",
    "4. **encouraging exploration:** To increase exploration I used a simple trick to encourage optimism in the face of uncertainty. Namely, I pretrained the network to output optimistic action-values across the state-space, which encourages exploration in the early phases of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup\n",
    "\n",
    "\n",
    " The heart of the algorithm can be found in:\n",
    "- [`train_utils.train`](https://github.com/richard-warren/rl_sandbox/blob/e56c44d74ddd47cbd6c2dc37753ba95896f9b81d/dm_control_tests/train_utils.py#L87), which trains an [`Agent`](https://github.com/richard-warren/rl_sandbox/blob/e56c44d74ddd47cbd6c2dc37753ba95896f9b81d/dm_control_tests/agents.py#L18) given an agent and a `dm_control` environment. \n",
    "- [`Agent.update`](https://github.com/richard-warren/rl_sandbox/blob/e56c44d74ddd47cbd6c2dc37753ba95896f9b81d/dm_control_tests/agents.py#L67), which selects minibatches and performs network updates.\n",
    "\n",
    "To increase training speed I found it helpful to:\n",
    "- *Train on the CPU rather than GPU.* The Q network is very small. My CPU was faster than the GPU unless batch sizes were really large.\n",
    "- *Perform forward passes on Numpy*. Network forward passes ended up being much faster with numpy than Tensorflow (again, unless batch sizes were really large).\n",
    "- *Train multiple agents in parallel*. Training results could be somewhat idiosyncratic even with the same hyperparameters, so I train 12 agents in parallel to make sure the results are robust.\n",
    "\n",
    "Below are some utility functions for plotting performance and showing rollouts for trained agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dm_control_tests import train_utils, plot_utils\n",
    "from dm_control_tests.agents import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from dm_control import suite\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "train_utils.disable_gpu()\n",
    "\n",
    "# plot performance over training, averaged across agents\n",
    "def plot_performance(x, data):\n",
    "    ax = plt.axes(xlabel='episode', ylabel='return',\n",
    "                  xlim=(x[0],x[-1]), ylim=(0,1000))\n",
    "    data = np.array(data)\n",
    "    if data.ndim==2:\n",
    "        mean = data.mean(0)\n",
    "        std = data.std(0)\n",
    "        ax.plot(x, data.T, color=(0,0,0), alpha=.15)\n",
    "        ax.plot(x, mean)\n",
    "        ax.fill_between(x, mean+std, mean-std, alpha=.15)\n",
    "    else:\n",
    "        ax.plot(x, data)\n",
    "\n",
    "# show a rollout for the agent with the best performance at the end of training\n",
    "def show_best_agent_rollout(agents_dir, framerate=30, epsilon=.05):\n",
    "    with open(os.path.join(agents_dir, 'training_data'), 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "    best_agent = np.argmax(training_data['avg_returns'][:,-1])\n",
    "    agent, metadata = train_utils.load_agent(\n",
    "        os.path.join(agents_dir, 'agent{:03d}'.format(best_agent)))\n",
    "    env = suite.load(*metadata['domain_and_task'])\n",
    "    return plot_utils.show_rollout_jupyter(agent, env, epsilon=epsilon, framerate=framerate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training\n",
    "Each action dimension is discretized into `action_grid=2` actions. This makes training fast, but it also means a given action can't be set to `0`. The replay buffer has length `buffer_length=50000` and is initialized using random actions prior to training. The Q network has hidden layers of size `units_per_layer=(12,24)`.  The target Q network is updated after every `q_update_interval=100` updates of the main Q network.\n",
    "\n",
    "The temporal resolution of `dm_control` is rather high. During exploration, especially with sparse rewards, the sequence of correctly chosen actions necessary to discover the reward may be quite long. I therefore repeat each action `action_repeats=2` times. Furthermore, I only perform network updates every `steps_per_update=4` iterations. The updates, rather than the physics simulations, were the speed bottleneck. Less frequent updates allow the agent to see more episodes with the same compute time.\n",
    "\n",
    "I train for `episodes=200` episodes with `batch_size=64`, acting $\\epsilon$-greedy with $\\epsilon$ linearly annealed from `epsilon_start=1` to `epsilon_final=.1` over `epsilon_final_episode=100` episodes.\n",
    "\n",
    "### evaluation\n",
    "Finally, every `eval_interval=10` episodes I evaluate performance by averaging the return from 5 rollouts with $\\epsilon = .05$. `nagents=12` agents are trained in parallel, and I plot the average += standard deviation of the return across these agents (with individual agents plotted as thin gray lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "agent_args = dict(\n",
    "    action_grid = 2,            # number of discrete actions per action dimension\n",
    "    units_per_layer = (12,24),  # hidden units per layer\n",
    "    buffer_length = 50000,\n",
    "    q_update_interval = 100,    # q updates per q_target update\n",
    "    learning_rate = .001,       # learning rate (adam optimizer)       \n",
    ")\n",
    "\n",
    "# training\n",
    "train_args = dict(\n",
    "    episodes = 200,\n",
    "    eval_interval = 10,\n",
    "    batch_size = 64,\n",
    "    action_repeats = 2,         # repeat each action this number of times during training\n",
    "    steps_per_update = 4,       # environment steps before updating q\n",
    "    gamma = .99,\n",
    "    epsilon_start = 1,\n",
    "    epsilon_final = .1,\n",
    "    epsilon_final_episode = 100,   # episode at which epsilon_final is reached\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solving simple tasks\n",
    "\n",
    "I worked on `dm_control` tasks with action space dimensionality <=2: `cartpole`, `ball_in_cup`, and `pendulum`. Reasonable policies seemed to emerge, although there was some variability across agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cartpole (balance)\n",
    "\n",
    "I use `train_utils.train_agents_parallel` to train multiple agents in parallel and save them to disk. Cartpole (balance) was solved fairly quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_num, avg_returns = train_utils.train_agents_parallel(\n",
    "    ('cartpole', 'balance'), agent_args, train_args, n_agents=12, n_workers=12,\n",
    "    save_dir='dm_control_tests/agents/cartpole_balance')\n",
    "plot_performance(episode_num, avg_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an $\\epsilon=.05$ rollout for the agent with the best performance at the end of training. It doesn't look like anything is happening, which is just what we want for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_agent_rollout('dm_control_tests/agents/cartpole_balance', epsilon=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cartpole (swingup)\n",
    "\n",
    "Swingup (non-sparse) is a bit more challenging, but DQN works here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_num, avg_returns = train_utils.train_agents_parallel(\n",
    "    ('cartpole', 'swingup'), agent_args, train_args, n_agents=12, n_workers=12,\n",
    "    save_dir='dm_control_tests/agents/cartpole_swingup')\n",
    "plot_performance(episode_num, avg_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_agent_rollout('dm_control_tests/agents/cartpole_swingup', epsilon=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ball in cup\n",
    "\n",
    "Ball in cup has two action dimensions. Performance across agents was more variable here, but performance was good overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_num, avg_returns = train_utils.train_agents_parallel(\n",
    "    ('ball_in_cup', 'catch'), agent_args, train_args, n_agents=12, n_workers=12,\n",
    "    save_dir='dm_control_tests/agents/ball_in_cup')\n",
    "plot_performance(episode_num, avg_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_agent_rollout('dm_control_tests/agents/ball_in_cup', epsilon=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pendulum\n",
    "\n",
    "To show how the action-value function evolves over time I train a single agent on `pendulum`. The state space is two dimensional, and with discretization $\\mathcal{A}=(-1,1)$. This means we can make nice heat maps that show action preference over the two-dimensional state space.\n",
    "\n",
    "First let's train the agent. I found it necessary to set $\\gamma=1$ (the reward is sparse here, and the agent has to swing back and forth several times to get it, so too much discounting can be problematic).\n",
    "\n",
    "In the `pendulum` environment the angle $\\theta$ is expressed with two values, $(\\cos(\\theta), \\sin(\\theta))$ (I'm guessing this is to get rid of the discontinuity in an angular representation?). I convert to $\\theta$ below so I can fit things into a nice two-dimensional plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load('pendulum', 'swingup')\n",
    "train_utils.rand_seed_reset(env, 0)\n",
    "agent = Agent(env.observation_spec(), env.action_spec(), **agent_args)\n",
    "train_utils.initialize_buffer(agent, env, verbose=False)\n",
    "\n",
    "# adjust training settings\n",
    "train_args_temp = train_args.copy()\n",
    "train_args_temp['gamma'] = 1\n",
    "\n",
    "# create state space grid points\n",
    "bins = 40\n",
    "angle_lims = (-np.pi/2, np.pi/2)\n",
    "vel_lims = (-10,10)\n",
    "axis_grids = [np.linspace(lims[0], lims[1], num=bins) for lims in (angle_lims, vel_lims)]\n",
    "grid = np.array(np.meshgrid(*axis_grids)).reshape(2, -1).T\n",
    "grid_predict = np.vstack((np.cos(grid[:,0]), np.sin(grid[:,0]), grid[:,1])).T  # first two columns are cos(x), sin(x) instead of x\n",
    "\n",
    "# this callback function will be evaluated over the course of training\n",
    "def compute_prediction_grid(agent, env):\n",
    "    q_map = agent.q.predict(grid_predict).reshape(bins, bins, 2)\n",
    "    return q_map\n",
    "\n",
    "episode_num, returns, q_maps = train_utils.train(\n",
    "    agent, env, **train_args_temp, verbose=False, callback=compute_prediction_grid)\n",
    "returns = [np.mean(r) for r in returns]\n",
    "plot_performance(episode_num, returns);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video below shows how preference for one action (blue) over the other (red) evolves over time. There is little preference initially (white), but a pretty structure emerges over training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_maps = [m[:,:,0] - m[:,:,1] for m in q_maps]\n",
    "abs_max = np.abs(np.array(preference_maps)).max()\n",
    "plot_utils.display_video(\n",
    "    preference_maps, framerate=2, is_plot=True,\n",
    "    imshow_args={'extent': angle_lims+vel_lims, 'cmap': 'bwr',\n",
    "                 'vmin': -abs_max, 'vmax': abs_max},\n",
    "    xlabel='angle', ylabel='velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function is both pretty and functional. See a sample rollout here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.show_rollout_jupyter(agent, env, epsilon=0, rand_seed=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, whever the starting state (which is randomized across episodes) is very low, the agent can't figure it out. In attemps to train for long periods of time (data not shown) I also found that performance can suffer after an intial period of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.show_rollout_jupyter(agent, env, epsilon=0, rand_seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# double q learning\n",
    "\n",
    "The max operation in Q-Learning can lead to overly optimistic value functions. In Double-Q Learning two Q functions are learned in parallel (typically on independent samples of data). One network is used to evaluate the actions selected by the other network. If the functions are $Q^a$ and $Q^b$, then the target used when updated $Q^a$ is:\n",
    "\n",
    "$$\n",
    "\\text{target}^a = R_{t+1} + \\gamma Q^b(S_{t+1}, \\text{argmax}_{a} Q^a(S_{t+1}, a) )\n",
    "$$\n",
    "\n",
    "[Double DQN](https://arxiv.org/abs/1509.06461) doesn't train networks on independent samples. Rather, it takes advantage of there already be two Q networks, which are (usually) different from one another. The target Q network, which should be more stable, is used for evaluating the actions selected by the main Q network.\n",
    "\n",
    "As in the original paper, I would like to check that using doubld DQN increases the accuracy of the action-value function. For DQN and double DQN agents I'll compute actual returns from a series of rollouts and compare these to returns estimated by their respective value-functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_args_dq = agent_args.copy()\n",
    "agent_args_dq['double_dqn'] = True\n",
    "\n",
    "episode_num, avg_returns = train_utils.train_agents_parallel(\n",
    "    ('ball_in_cup', 'catch'), agent_args_dq, train_args, n_agents=12, n_workers=12,\n",
    "    save_dir='dm_control_tests/agents/ball_in_cup_dq')\n",
    "plot_performance(episode_num, avg_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_agent_rollout('dm_control_tests/agents/ball_in_cup_dq', epsilon=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll assess evaluation accuracy for DQN and Double DQN by comparing actual returns with returns predicted by the Q function. For cartpole (balance) I'll collect returns from 10 agents with 5 rollouts each (for both DQN and Double DQN agents) and plot actual vs. predicted return densities.\n",
    "\n",
    "One detail: Episodes being truncated after 1000 steps will cause later returns to be underestimated. Therefore, when computing returns the final reward $R_T$ is given the remaining weight in the gamma distribution, e.g.\n",
    "\n",
    "\\begin{align}\n",
    "G_t &= \\sum_{i=t}^{T-1} \\gamma^{i-t} R_i + \\sum_{i=T}^\\infty \\gamma^{i-t} R_T \\\\\n",
    "&= \\sum_{i=t}^{T-1} \\gamma^{i-t} R_i + \\gamma^{T-t} \\sum_{i=0}^\\infty \\gamma^{i} R_{T} \\\\ \n",
    "&= \\sum_{i=t}^{T-1} \\gamma^{i-t} R_i + \\frac{\\gamma^{T-t} R_{T}}{1-\\gamma}\n",
    "\\end{align}\n",
    "\n",
    "I compute returns \"backwards\" using $G_t = R_t + \\gamma G_{t+1}$. This means the above equation can be implemented by settings $G_T = \\frac{R_T}{1-\\gamma}$ and then computing recursively thereafter. I still want to avoid returns that start close to the end of an episode, so I only included returns for which $\\gamma^{T-t}<.01$ (so the truncated portion of the $\\gamma$ distribution is negligible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare DQN and Double DQN evaluation accuracy\n",
    "n_agents = 10\n",
    "n_rollouts = 5  # per agent\n",
    "shake_things_up = False\n",
    "gamma_thresh = .01\n",
    "env = suite.load('cartpole', 'swingup')\n",
    "agent_q  = 'dm_control_tests/agents/cartpole_swingup/'\n",
    "agent_dq = 'dm_control_tests/agents/cartpole_swingup_dq/'\n",
    "\n",
    "\n",
    "# returns will be truncated near end of episodes, so only consider early returns\n",
    "# with enough samples s.t. discounting makes further samples negligible\n",
    "last_ind = int(env._step_limit - np.argmax(\n",
    "    np.power(train_args['gamma'], np.arange(env._step_limit)) < gamma_thresh))\n",
    "\n",
    "q_data = {'returns': [], 'predictions': []}\n",
    "dq_data = {'returns': [], 'predictions': []}\n",
    "\n",
    "for agent_name, data in zip((agent_q, agent_dq), (q_data, dq_data)):  # for dqn, double dqn\n",
    "    for agent_num in tqdm(range(n_agents)):\n",
    "        agent = train_utils.load_agent(os.path.join(agent_name, 'agent{:03d}'.format(agent_num)))[0]\n",
    "        for i in range(n_rollouts):\n",
    "            predictions, rewards = [], []\n",
    "            time_step = env.reset()\n",
    "            if shake_things_up:\n",
    "                env.physics.data.qpos[:] = np.random.randn(env.physics.model.nq)\n",
    "\n",
    "            # perform rollout and get return predictions\n",
    "            while not time_step.last():\n",
    "                action = agent.select_action(time_step, train_args['epsilon_final'])\n",
    "                action_idx = agent.index_from_action(action)\n",
    "                time_step = env.step(action)\n",
    "                rewards.append(time_step.reward)\n",
    "                x = agent.get_observation_vector(time_step)\n",
    "                predictions.append(agent.predict(x, agent.q).max())\n",
    "\n",
    "            # compute actual returns in reverse\n",
    "            returns = [rewards[-1] * (1/(1-train_args['gamma']))]  # multiply final return by integral of gamma function to account for episode termination\n",
    "            for r in reversed(rewards[:-1]):\n",
    "                returns.insert(0, r + train_args['gamma']*returns[0])  # G_t = G_t + \\gamma G_{t+1}\n",
    "\n",
    "            # store\n",
    "            data['returns'] += returns[:last_ind]\n",
    "            data['predictions'] += predictions[:last_ind]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are densities for actual vs. predicted return. Both DQN and Double DQN were fairly, and comparably, accurate. I suspect the benefits of Double DQN are minimal because I copy weights to the Q target network every `100` updates, which is quite frequent compared to the `10000` used in the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "for i, data, title in zip(range(2), (q_data, dq_data), ('DQN', 'Double DQN')):\n",
    "#     ax[i].scatter(data['returns'], data['predictions'], alpha=.1)\n",
    "    ax[i].set(xlim=(0,100), ylim=(0,100), xlabel='actual return', ylabel='predicted return', title=title)\n",
    "    sns.kdeplot(data['returns'], data['predictions'], ax=ax[i], shade=True, shade_lowest=False, cmap='Reds')\n",
    "    ax[i].plot([0,100], [0,100], color=(.2,.2,.2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimistic initializations\n",
    "for sparse reward agent needs to accidentally discover correct action sequence // can be very hard when that sequence is long // initializing optimisitic values of q can hopefully encourage initial exploration and speed up learning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: determine if optimism helps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train agent in parallel\n",
    "\n",
    "agent_args_temp = agent_args.copy()\n",
    "train_args_temp = train_args.copy()\n",
    "\n",
    "agent_args_temp['double_dqn'] = True\n",
    "train_args_temp['gamma'] = 1\n",
    "\n",
    "\n",
    "episode_num, avg_returns = train_utils.train_agents_parallel(\n",
    "    ('pendulum', 'swingup'), agent_args_temp, train_args_temp, n_agents=12, n_workers=12,\n",
    "    save_dir='dm_control_tests/agents/pendulum_swingup_dq')\n",
    "plot_performance(episode_num, avg_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reloading previous training data...\n",
    "with open('dm_control_tests/agents/pendulum_swingup/training_data', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "episode_num = training_data['episode_num']\n",
    "avg_returns = training_data['avg_returns']\n",
    "plot_performance(episode_num, avg_returns)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMIMwli/Fl2rto2fCxZqS+/",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dqn_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
